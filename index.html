<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="NeuralFieldsBeyondCams">
  <meta name="keywords" content="NeuralFieldsBeyondCams">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="./static/img/neural_fields.webp" />
  <title>Neural Fields Beyond Conventional Cameras</title>
  
  <meta property="og:description" content="Neural Fields Beyond Conventional Cameras"/>
  <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
  <script defer src="assets/fontawesome.all.min.js"></script>

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:creator" content="@mmalex">
  <meta name="twitter:title" content="Neural Fields Beyond Conventional Cameras">
  <meta name="twitter:description" content="The first workshop on neural fields beyond conventional cameras, hosted at ECCV 2024.">

  <link rel="icon" href="./static/img/camera.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="css/style.css"> <!-- Resource style -->
  <script src="js/modernizr.js"></script> <!-- Modernizr -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>

  <style>
    .rcorners1 {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px;
      font-size: 120%;
      color: #5c5c5c;
    }

    .button {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px 15px 5px 15px;
    }

    .dropdown {
      position: relative;
      display: inline-block;
    }

    .dropdown-content {
      display: none;
      position: absolute;
      /* background-color: #f9f9f9; */
      min-width: 140px;
      box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
      padding: 5px 15px 5px 15px;
      z-index: 1;
    }

    .dropdown:hover .dropdown-content {
      display: block;
    }
  </style>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
              <p class="title is-1 publication-title">1<sup>st</sup> Workshop on Neural Fields Beyond Conventional Cameras
              </p>
            </h1>
            <h1 class="is-is-5" style="color: #5c5c5c;">in conjunction with ECCV 2024, Milan, Italy.</h1>
            <b>Date: TBD</b>
            <br>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <a href="#schedule" class="button">Schedule</a>
          <a href="#speakers" class="button">Speakers</a>
          <a href="#call4paper" class="button">Call for Papers</a>
          <a href="#relatedwork" class="button">Related Work</a>
          <!-- <a href="#accepted" class="button">Accepted Papers</a> -->
          <!-- <a href="#challenge" class="button">Challenge</a> -->
          <a href="#organizers" class="button">Organizers</a>
          <!-- <div class="dropdown">
            <span class="button"><i class="fa fa-bars"></i></span>
            <div class="dropdown-content">
              <a href="index.html">CVPR 2024</a>
              <a href="index_iccv23.html">ICCV 2023</a>
            </div> -->
        </h2>
        
      </div>
    </div>
  </section>

  <!-- ############################## -->
  <!-- motivation -->
  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="Motivation">
        <div class="container is-max-desktop content">
          <h2 class="title">Motivation üí°</h2>
          <div class="content has-text-justified">
            Neural fields have been widely adopted for learning novel view synthesis and 3D reconstruction from RGB images by modelling transport of light in the visible spectrum. This workshop focuses on <em>neural fields beyond conventional cameras</em>, including (1) learning neural fields from data from different sensors across the electromagnetic spectrum and beyond, such as lidar, cryo-electron microscopy (cryoEM), thermal, event cameras, acoustic, and more, and (2) modelling associated physics-based differentiable forward models and/or the physics of more complex light transport (reflections, shadows, polarization, diffraction limits, optics, scattering in fog or water, etc.). Our goal is to bring together a diverse group of researchers using neural fields across sensor domains to foster learning and discussion in this growing area. 
          </div>
        </div>
      </section>

    <section class="section" style="margin-top: -50px">
      <div class="container is-max-desktop">
        <!-- ############################## -->
        <!-- scheduler -->
        <section class="section" id="schedule">
          <div class="container is-max-desktop content">
            <h2 class="title">Schedule ‚è∞ (tentative)</h2>
            <div class="content has-text-justified">
              <table class="table table-striped">
                <tr>
                  <td width="130">13:15 - 13:20</td>
                  <td width="300" style="background-color:#e4ffc2">Welcome & Introduction</td>
                  <td></td>
                </tr>
                <tr>
                  <td>13:20 - 13:45</td>
                  <td style="background-color:#cae1ff">Keynote 1 <b></b></td>
                  <td></td>
                </tr>
                <tr>
                  <td>13:45 - 14:10</td>
                  <td style="background-color:#cae1ff">Keynote 2 <b></b></td>
                  <td></td>
                </tr>
                <tr>
                  <td>14:10 - 14:35</td>
                  <td style="background-color:#cae1ff">Keynote 3</td>
                  <td></td>
                </tr>
                <tr>
                  <td>14:35 - 14:45</td>
                  <td style="background-color:#ffe0c6">Paper Spotlight Presentation 1<b></b></td>
                  <td></td>
                </tr>
                <tr>
                  <td>14:45 - 14:55</td>
                  <td style="background-color:#ffe0c6">Paper Spotlight Presentation 2<b></b></td>
                  <td></td>
                </tr>
                <tr>
                  <td>14:55 - 15:05</td>
                  <td style="background-color:#ffe0c6">Paper Spotlight Presentation 3<b></b></td>
                  <td></td>
                </tr>
                <tr>
                  <td>15:05 - 15:50</td>
                  <td style="background-color:#e4ffc2">Poster Session & Coffee Break</td>
                  <td></td>
                </tr>
                <tr>
                  <td>15:50 - 16:15</td>
                  <td style="background-color:#cae1ff">Keynote 4<b></b></td>
                  <td></td>
                </tr>
                <tr>
                  <td>16:15 - 16:40</td>
                  <td style="background-color:#cae1ff">Keynote 5<b></b></td>
                  <td></td>
                </tr>
                <tr>
                  <td>16:40 - 17:05</td>
                  <td style="background-color:#cae1ff">Keynote 6</td>
                  <td></td>
                </tr>                  
                <tr>
                  <td>17:05 - 17:40</td>
                  <td style="background-color:#e4ffc2">Panel Discussion</td>
                  <td></td>
                </tr>
              </table>
            </div>
          </div>
        </section>

        <!-- ############################## -->
        <!-- invited speakers -->
        <section class="section" id="Invited Speakers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="speakers">Invited Speakers üßë‚Äçüè´ (tentative)</h2>

            <a href="https://davidlindell.com" target="_blank">
              <div class="card">
                <div class="card-content">
                  <div class="columns is-vcentered">
                    <div class="column is-one-quarter">
                      <figure class="image is-128x128">
                        <img class="is-rounded" src="static/img/david_lindell.jpeg">
                      </figure>
                    </div>
                    <div class="column">
                      <p class="title is-4">David Lindell</p>
                      <p class="subtitle is-6">University of Toronto</p>
                    </div>
                  </div>
                  <div class="content">
                    David Lindell is an Assistant Professor in the Department of Computer Science at the University of Toronto and founding member of the Toronto Computational Imaging Group. His work is at the intersection of machine learning, computational imaging, and computer vision. Along these lines he has worked on next-generation computational imaging systems for imaging around corners and through scattering media, and new machine learning algorithms for representing and processing signals. His work is relevant to a broad range of applications in computer graphics, vision, and remote sensing.
                  </div>
                </div>
              </div>
            </a>  
            <a href="https://www.cs.princeton.edu/~zhonge" target="_blank">
              <div class="card">
                <div class="card-content">
                  <div class="columns is-vcentered">
                    <div class="column is-one-quarter">
                      <figure class="image is-128x128">
                        <img class="is-rounded" src="static/img/ellen_zhong.png">
                      </figure>
                    </div>
                    <div class="column">
                      <p class="title is-4">Ellen Zhong</p>
                      <p class="subtitle is-6">Princeton</p>
                    </div>
                  </div>
                  <div class="content">
                    Ellen Zhong is an Assistant Professor of Computer Science at Princeton University. She is interested in problems at the intersection of AI and biology. Her research develops machine learning methods for computational and structural biology problems with a focus on protein structure determination with cryo-electron microscopy (cryo-EM). She obtained her Ph.D. from MIT in 2022, advised by Bonnie Berger and Joey Davis, where she developed deep learning algorithms for 3D reconstruction of dynamic protein structures from cryo-EM images. She has interned at DeepMind (AlphaFold team) and previously worked on molecular dynamics algorithms at D. E. Shaw Research. 
                  </div>
                </div>
              </div>
            </a>
            <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">
              <div class="card">
                <div class="card-content">
                  <div class="columns is-vcentered">
                    <div class="column is-one-quarter">
                      <figure class="image is-128x128">
                        <img class="is-rounded" src="static/img/daniel_cremers.png">
                      </figure>
                    </div>
                    <div class="column">
                      <p class="title is-4">Daniel Cremers</p>
                      <p class="subtitle is-6">TU Munich</p>
                    </div>
                  </div>
                  <div class="content">
                    Daniel Cremers is a Professor at Technical University of Munich where he holds the Chair of Computer Vision and Artificial Intelligence. His publications received several awards, including the 'Best Paper of the Year 2003' (Int. Pattern Recognition Society), the 'Olympus Award 2004' (now called 'German Pattern Recognition Award') and the '2005 UCLA Chancellor's Award for Postdoctoral Research'. For pioneering research he received a Starting Grant (2009), two Proof of Concept Grants (2014 \& 2018), a Consolidator Grant (2015) and an Advanced Grant (2020) by the European Research Council. In December 2010 he was listed among "Germany's top 40 researchers below 40" (Capital). On March 1st 2016, Prof. Cremers received the Gottfried Wilhelm Leibniz Award, the biggest award in German academia. In 2022 and 2023, he was listed among the top 10 most influential scholars in robotics of the last decade. He serves as co-founder, advisor and business angel to several startups.
                  </div>
                </div>
              </div>
            </a>
            <a href="https://jonbarron.info" target="_blank">
              <div class="card">
                <div class="card-content">
                  <div class="columns is-vcentered">
                    <div class="column is-one-quarter">
                      <figure class="image is-128x128">
                        <img class="is-rounded" src="static/img/jon_barron.jpeg">
                      </figure>
                    </div>
                    <div class="column">
                      <p class="title is-4">Jon Barron</p>
                      <p class="subtitle is-6">Google</p>
                    </div>
                  </div>
                  <div class="content">
                    Jon Barron is a senior staff research scientist at Google Research, where he works on computer vision and machine learning. He received a PhD in Computer Science from the University of California, Berkeley in 2013, where he was advised by Jitendra Malik, and he received a Honours BSc in Computer Science from the University of Toronto in 2007. He received a National Science Foundation Graduate Research Fellowship in 2009, the C.V. Ramamoorthy Distinguished Research Award in 2013, and the PAMI Young Researcher Award in 2020. His works have received awards at ECCV 2016, TPAMI 2016, ECCV 2020, ICCV 2021, CVPR 2022, the 2022 Communications of the ACM, and ICLR 2023.
                  </div>
                </div>
              </div>
            </a> 
            <a href="https://www.viseaon.haifa.ac.il/" target="_blank">
              <div class="card">
                <div class="card-content">
                  <div class="columns is-vcentered">
                    <div class="column is-one-quarter">
                      <figure class="image is-128x128">
                        <img class="is-rounded" src="static/img/tali_treibitz.webp">
                      </figure>
                    </div>
                    <div class="column">
                      <p class="title is-4">Tali Treibitz</p>
                      <p class="subtitle is-6">University of Haifa</p>
                    </div>
                  </div>
                  <div class="content">
                    Tali Treibitz is heading the Viseaon marine imaging lab in the School of Marine Sciences in the University of Haifa since 2014. She received her PhD degree in electrical engineering from the Technion-Israel Institute of Technology in 2010. Between 2010-2013 she was a post-doctoral researcher in the department of computer science and engineering, in the University of California, San Diego and in the Marine Physical Lab in Scripps Institution of Oceanography. Her lab focuses on cutting edge research in underwater computer vision, scene, color and 3D reconstruction, automatic analysis of scenes, and autonomous decision making based on visual input.
                  </div>
                </div>
              </div>
            </a>  
            <a href="https://www.cs.columbia.edu/~vondrick" target="_blank">
              <div class="card">
                <div class="card-content">
                  <div class="columns is-vcentered">
                    <div class="column is-one-quarter">
                      <figure class="image is-128x128">
                        <img class="is-rounded" src="static/img/carl_vondrick.jpeg">
                      </figure>
                    </div>
                    <div class="column">
                      <p class="title is-4">Carl Vondrick</p>
                      <p class="subtitle is-6">Columbia University</p>
                    </div>
                  </div>
                  <div class="content">
                    Carl Vondrick is an Associate professor of computer science at Columbia University. His research focuses on computer vision and machine learning. By training machines to observe and interact with their surroundings, his group aims to create robust and versatile models for perception. They often develop visual models that capitalize on large amounts of unlabeled data and transfer across tasks and modalities. Other interests include sound and language, interpretable models and high-level reasoning. Recent work includes 3D reconstruction from shadows and thermal reflections. 
                  </div>
                </div>
              </div>
            </a>
            
          </div>
        </section>

        <!-- ############################## -->
        <!-- call for papers -->
        <section class="section" id="Call for papers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="call4paper">Call for Papers üì¢üì¢</h2>
            This workshop aims to bring together a diverse group of researchers using neural fields across sensor domains to foster learning and discussion in this growing area. We welcome paper submissions on all topics related to neural fields beyond conventional cameras. Accepted papers will be posted on the workshop website, but will not be part of the ECCV proceedings. Relevant topics for this workshop include but are not limited to:
            <ul>
              <li>Neural field-based reconstruction and view synthesis using non-RGB sensor measurements (LiDAR, Thermal, Event, CT, MRI, Ultrasound, Cryo-EM, Sonar, etc)</li>
              <li>Neural fields for computational imaging</li>
              <li>Neural fields for sensor modelling and calibration</li>
              <li>Neural fields for modelling visual cues (shadows, reflections, polarization, etc)</li>
              <li>Applications of the above to autonomous vehicles, AR/VR/XR, robotics, medicine, scientific dis-
                covery, and beyond</li>
            </ul>

            <h2 class="title" id="">Style and Author Instructions</h2>
            <ul>
              <li>
                <b>Paper Length:</b> We ask authors to use the official ECCV 2024 template and limit submissions to 4-8 pages, excluding references.
              </li>
              <li>
                <b>Dual Submissions:</b> The workshop is non-archival. In addition, in light of the new single-track policy of ECCV 2024, we encourage papers accepted to ECCV 2024 to present at our workshop.
              </li>
              <li>
                <b>Presentation Forms:</b> All accepted papers will get poster presentations during the workshop; selected papers will get oral presentations.
              </li>
            </ul>
            
            <p class="text-justify">
              All submissions should anonymized. Papers with more than 4 pages
              (excluding references) will be reviewed as long papers, and papers with
              more than 8 pages (excluding references) will be rejected without
              review. Supplementary material is optional with supported formats: pdf, mp4 and zip.
            </p>
          <p>All submissions should adhere to the <a
                  href="https://eccv.ecva.net/Conferences/2024/SubmissionPolicies">ECCV 2024 submission
                  guidelines</a>, wherever applicable.
          </p>
          <p>
              <strong>Submission Portal: </strong><a href="https://openreview.net/group?id=thecvf.com/ECCV/2024/Workshop/NFBCC">OpenReview</a>
          </p>
          <p><strong>Paper Review Timeline:</strong>
          <table class="table">
              <tr>
                  <th scope="col">Paper Submission and supplemental material deadline</th>
                  <td> July 19, 2024 (AoE time)</td>
              </tr>
              <tr>
                  <th scope="col">Notification to authors</th>
                  <td> August 16, 2024 (AoE time) </td>
              </tr>
              <tr>
                  <th scope="col">Camera ready deadline</th>
                  <td> August 23, 2024 (AoE time) </td>
              </tr>
          </table>

          </div>
        </section>






        <!-- ############################## -->
        <!-- related work -->
        <section class="section" id="Related works">
          <div class="container is-max-desktop content">
            <h2 class="title" id="relatedwork">Related Works üßë‚Äçü§ù</h2>
            Below is a collection of related works in the field of neural fields beyond conventional cameras.
            Please
            feel free to get in touch to add other works as well.
            <ul>
              <li><a href="https://arxiv.org/abs/2312.14239">	
                PlatoNeRF: 3D Reconstruction in Plato's Cave via Single-View Two-Bounce Lidar</a> CVPR 2024
              </li>
              <li><a href="https://arxiv.org/abs/2312.05247">Dynamic LiDAR Re-simulation using Compositional Neural Fields</a> CVPR 2024
              </li>
              <li><a href="https://arxiv.org/abs/2305.16321">Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</a> CVPR 2024 </li>
              <li><a href="https://arxiv.org/abs/2208.11300">E-NeRF: Neural Radiance Fields from a Moving Event Camera</a> RA-L 2023 </li>
              <li><a href="https://arxiv.org/abs/2301.10520">Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging</a> MIDL 2023 </li>
              <li><a href="https://arxiv.org/abs/2307.09555">Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction</a> NeurIPS 2023 </li>
              <li><a href="https://arxiv.org/abs/2305.01643">	
                Neural LiDAR Fields for Novel View Synthesis</a> ICCV 2023
              </li>
              <li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shandilya_Neural_Fields_for_Structured_Lighting_ICCV_2023_paper.pdf">Neural Fields for Structured Lighting</a> ICCV 2023 </li>
              <li><a href="https://arxiv.org/abs/2212.04531">ORCa: Glossy Objects as Radiance Field Cameras</a> CVPR 2023 </li>
              <li><a href="https://arxiv.org/abs/2305.01652">Humans as Light Bulbs: 3D Human Reconstruction from Thermal Reflection</a> CVPR 2023 </li>
              <li><a href="https://arxiv.org/abs/2304.07743">SeaThru-NeRF: Neural Radiance Fields in Scattering Media</a> CVPR 2023 </li>
              <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/20171">Neural Interferometry: Image Reconstruction from Astronomical Interferometers Using Transformer-Conditioned Neural Fields</a> AAAI 2022 </li>
              <li><a href="https://arxiv.org/abs/2204.00628">Learning Neural Acoustic Fields</a> NeurIPS 2022 </li>
              <li><a href="https://arxiv.org/abs/2203.13458">	
                PANDORA: Polarization-Aided Neural Decomposition Of Radiance</a> ECCV 2022
              </li>                
              <li><a href="https://arxiv.org/abs/2202.01020">Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray</a> EMBC 2022
              </li>
              <li><a href="https://arxiv.org/abs/2109.15271">T√∂RF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis</a> NeurIPS 2021 </li>
              <li><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhong_CryoDRGN2_Ab_Initio_Neural_Reconstruction_of_3D_Protein_Structures_From_ICCV_2021_paper.pdf">CryoDRGN2: Ab initio neural reconstruction of 3D protein structures from real cryo-EM images</a> ICCV 2021 </li>
              <br>
            </ul>
            and many more ...


            </ul>
          </div>
        </section>
          <!-- <section class="section" id="Papers">
            <div class="container is-max-desktop content">
              <h2 class="title" id="dates">Important Dates üóìÔ∏è</h2>
              <ul>
                <li><b>Paper Track</b>: We accept novel full 8-page papers for publication in the proceedings, and
                  either shorter
                  4-page extended abstracts or 8-page papers of novel or previously published work that will not
                  be included in
                  the
                  proceedings. All submissions have to follow the <a
                    href="https://cvpr.thecvf.com/Conferences/2024/AuthorGuidelines">CVPR 2024 author
                    guidelines.</a>
                </li>
                <ul>
                  <li><b>Submission Portal</b>: <a href="https://cmt3.research.microsoft.com/OpenSUN3D2024">CMT</a></li>
                  <li><b>Paper Submission Deadline</b>: April 1, 2024 (23:59 Pacific Time)</li>
                  <li><b>Notification to Authors</b>: April 9, 2024</li>
                  <li><b>Camera-ready submission</b>: April 14, 2024</li>
                </ul>
                <li><b>Challenge Track 1</b>: <a
                    href="https://opensun3d.github.io/cvpr24-challenge/track_1">Open-vocabulary 3D object instance
                    search</a></li>
                <ul>
                  <li><b>Submission Portal</b>: EvalAI</li>
                  <li><b>Data Instructions & Helper Scripts</b>: April 15, 2024</li>
                  <li><b>Dev Phase Start</b>: April 15, 2024</li>
                  <li><b>Submission Portal Start</b>: April 15, 2024</li>
                  <li><b>Test Phase Start</b>: May 1, 2024</li>
                  <li><b>Test Phase End</b>: June 8, 2024 (23:59 Pacific Time)</li>

                </ul>
                <li><b>Challenge Track 2</b>: <a
                    href="https://opensun3d.github.io/cvpr24-challenge/track_2">Open-vocabulary 3D affordance
                    grounding</a></li>
                <ul>
                  <li><b>Submission Portal</b>: EvalAI</li>
                  <li><b>Data Instructions & Helper Scripts</b>: April 15, 2024</li>
                  <li><b>Dev Phase Start</b>: April 15, 2024</li>
                  <li><b>Submission Portal Start</b>: April 15, 2024</li>
                  <li><b>Test Phase Start</b>: May 1, 2024</li>
                  <li><b>Test Phase End</b>: June 8, 2024 (23:59 Pacific Time)</li>

                </ul>
              </ul>
            </div>
          </section> -->

          

        <!-- ############################## -->
        <!-- organizers -->
        <section class="section" id="Organizers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="organizers">Organizers</h2>
            
            <div class="columns is-centered is-variable is-0">

              <div class="column is-one-quarter">
                <a href="https://tzofi.github.io">
                  <div class="card">
                    <div class="card-image">
                      <figure class="image">
                        <img class="is-rounded" src="./static/img/Tzofi.jpg" alt="Image of Tzofi Klinghoffer">
                      </figure>
                    </div>
                    <div class="card-content">
                      <div class="media">
                        <div class="media-content" style="overflow-x: unset;">
                          <p class="title is-7 is-spaced">Tzofi Klinghoffer</p>
                          <p class="subtitle is-7">MIT Media Lab </p>
                        </div>
                      </div>
                    </div>
                  </div>
                </a>
              </div>
              <div class="column is-one-quarter">
                <a href="https://shengyuh.github.io">
                  <div class="card">
                    <div class="card-image">
                      <figure class="image">
                        <img class="is-rounded" src="./static/img/Shengyu.jpeg" alt="Image of Shengyu Huang">
                      </figure>
                    </div>
                    <div class="card-content">
                      <div class="media">
                        <div class="media-content" style="overflow-x: unset;">
                          <p class="title is-7 is-spaced">Shengyu Huang</p>
                          <p class="subtitle is-7">ETH Z√ºrich</p>
                        </div>
                      </div>
                    </div>
                  </div>
                </a>
              </div>
              <div class="column is-one-quarter">
                <a href="">
                  <div class="card">
                    <div class="card-image">
                      <figure class="image">
                        <img class="is-rounded" src="./static/img/Daniel.jpg" alt="Placeholder image">
                      </figure>
                    </div>
                    <div class="card-content">
                      <div class="media">
                        <div class="media-content" style="overflow-x: unset;">
                          <p class="title is-7 is-spaced">Daniel Gilo</p>
                          <p class="subtitle is-7">Technion</p>
                        </div>
                      </div>
                    </div>
                  </div>
                </a>
              </div>
              <div class="column is-one-quarter">
                <a href="https://www.media.mit.edu/people/ktiwary/overview/">
                  <div class="card">
                    <div class="card-image">
                      <figure class="image">
                        <img class="is-rounded" src="./static/img/Kushagra.png" alt="Image of Kushagra Tiwary">
                      </figure>
                    </div>
                    <div class="card-content">
                      <div class="media">
                        <div class="media-content" style="overflow-x: unset;">
                          <p class="title is-7 is-spaced">Kushagra Tiwary</p>
                          <p class="subtitle is-7">MIT Media Lab</p>
                        </div>
                      </div>
                    </div>
                  </div>
                </a>
              </div>
              

            </div>

            <div class="columns is-centered is-variable is-0">

              <div class="column is-one-fifth">
                <a href="https://akshatdave.github.io">
                  <div class="card">
                    <div class="card-image">
                      <figure class="image">
                        <img class="is-rounded" src="./static/img/Akshat.jpg" alt="Image of Akshat Dave">
                      </figure>
                    </div>
                    <div class="card-content">
                      <div class="media">
                        <div class="media-content" style="overflow-x: unset;">
                          <p class="title is-7 is-spaced">Akshat Dave</p>
                          <p class="subtitle is-7">MIT Media Lab</p>
                        </div>
                      </div>
                    </div>
                  </div>
                </a>
              </div>
              <div class="column is-one-fifth">
                <a href="https://lingjie0206.github.io">
                  <div class="card">
                    <div class="card-image">
                      <figure class="image">
                        <img class="is-rounded" src="./static/img/Lingjie.jpg" alt="Image of Lingjie Liu">
                      </figure>
                    </div>
                    <div class="card-content">
                      <div class="media">
                        <div class="media-content" style="overflow-x: unset;">
                          <p class="title is-7 is-spaced">Lingjie Liu</p>
                          <p class="subtitle is-7">Univ. of Pennsylvania</p>
                        </div>
                      </div>
                    </div>
                  </div>
                </a>
              </div>
              <div class="column is-one-fifth">
                <a href="https://jamestompkin.com">
                  <div class="card">
                    <div class="card-image">
                      <figure class="image">
                        <img class="is-rounded" src="./static/img/James.jpg" alt="Image of James Tompkin">
                      </figure>
                    </div>
                    <div class="card-content">
                      <div class="media">
                        <div class="media-content" style="overflow-x: unset;">
                          <p class="title is-7 is-spaced">James Tompkin</p>
                          <p class="subtitle is-7">Brown University</p>
                        </div>
                      </div>
                    </div>
                  </div>
                </a>
              </div>
              <div class="column is-one-fifth">
                <a href="https://orlitany.github.io">
                  <div class="card">
                    <div class="card-image">
                      <figure class="image">
                        <img class="is-rounded" src="./static/img/orlitany_square.jpg" alt="Image of Or Litany">
                      </figure>
                    </div>
                    <div class="card-content">
                      <div class="media">
                        <div class="media-content" style="overflow-x: unset;">
                          <p class="title is-7 is-spaced">Or Litany</p>
                          <p class="subtitle is-7">NVIDIA, Technion</p>
                        </div>
                      </div>
                    </div>
                  </div>
                </a>
              </div>
              <div class="column is-one-fifth">
                <a href="https://www.media.mit.edu/people/raskar/overview/">
                  <div class="card">
                    <div class="card-image">
                      <figure class="image">
                        <img class="is-rounded" src="./static/img/Ramesh.jpg" alt="Image of Ramesh Raskar">
                      </figure>
                    </div>
                    <div class="card-content">
                      <div class="media">
                        <div class="media-content" style="overflow-x: unset;">
                          <p class="title is-7 is-spaced">Ramesh Raskar</p>
                          <p class="subtitle is-7">MIT Media Lab</p>
                        </div>
                      </div>
                    </div>
                  </div>
                </a>
              </div>
              

            </div>

          </div>
        </section>

          <!-- <section class="section" id="Sponsors">
        <div class="container is-max-desktop content">
          <h2 class="title">Sponsors </h2>
        </div>
      </section> -->

          <footer class="footer">
            <div class="container">
              <div class="content">
                <p>
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  <br />
                  It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>.
                  We would like to thank Utkarsh Sinha and Keunhong Park.
                </p>
              </div>
            </div>
          </footer>
</body>
<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script> <!-- Resource jQuery -->
<script src="js/main.js"></script> <!-- Resource jQuery -->

</html>
